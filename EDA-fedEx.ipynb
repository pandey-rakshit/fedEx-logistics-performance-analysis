{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77412171",
   "metadata": {},
   "source": [
    "# Project Name "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddb83ac",
   "metadata": {},
   "source": [
    "**Project Type -** EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e9c0a",
   "metadata": {},
   "source": [
    "**Contribution -** Individual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196c6294",
   "metadata": {},
   "source": [
    "**Rakshit Pandey**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5715c3b",
   "metadata": {},
   "source": [
    "## Project Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73838af7",
   "metadata": {},
   "source": [
    "**Write the summary here within 500-600 words**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5181ca2",
   "metadata": {},
   "source": [
    "## Github Link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedebceb",
   "metadata": {},
   "source": [
    "**Provide your Github Link here.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4bf9bf",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0bfde9",
   "metadata": {},
   "source": [
    "**Write Problem Statement Here.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db63441",
   "metadata": {},
   "source": [
    "## Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9762f35",
   "metadata": {},
   "source": [
    "**Write Business Context Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660839d6",
   "metadata": {},
   "source": [
    "### Define Your Business Objective ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1812b7",
   "metadata": {},
   "source": [
    "**Answer Here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2b6f32",
   "metadata": {},
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2ecc9",
   "metadata": {},
   "source": [
    "**Dataset** :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df01eb",
   "metadata": {},
   "source": [
    "**Data overview** :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb02058",
   "metadata": {},
   "source": [
    "## Let's Begin !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5417e5f5",
   "metadata": {},
   "source": [
    "### Project Setup -- constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d45528fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = '' # (support raw files only) - extension - {csv, xlsx, json, parquet}\n",
    "skew_threshold = 0.5\n",
    "outlier_threshold = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50bfa90-61d2-4ad9-9697-5f4cfbef8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734b1e81",
   "metadata": {},
   "source": [
    "### Prerequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f8d0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Installation\n",
    "\n",
    "!pip install pandas numpy scipy scikit-learn matplotlib seaborn wordcloud --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37851b46",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bcf5e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical operations, including mathematical functions like mean, median, sqrt, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt  # For creating a wide range of visualizations, such as bar plots, histograms, scatter plots, etc.\n",
    "import seaborn as sns  # For advanced statistical visualizations, including pairplots, violin plots, and heatmaps\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f584d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Setting styles for plots\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c8457",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26144e4-47ed-4c1a-b83c-f73396a421f3",
   "metadata": {},
   "source": [
    "#### Visualizatioin - Utitlity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eef7a499-dbf8-4e42-8fc5-988fb80d7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart visualzation function\n",
    "\n",
    "plot_functions = {\n",
    "    'scatter': sns.scatterplot,\n",
    "    'line': sns.lineplot,\n",
    "    'bar': sns.barplot,\n",
    "    'box': sns.boxplot,\n",
    "    'hist': sns.histplot,\n",
    "    'pie': plt.pie,\n",
    "    'count': sns.countplot,\n",
    "    'heatmap': sns.heatmap\n",
    "}\n",
    "\n",
    "\n",
    "def visualize_chart(chart_objs, nrows=1, ncols=1, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a custom visualization chart with optional subplots.\n",
    "\n",
    "    Parameters:\n",
    "    - chart_objs (list): List of dictionaries, where each dictionary contains\n",
    "                          chart-specific information like 'plot_function', 'titles', etc.\n",
    "    - nrows (int): Number of rows for the subplot grid. Default is 1.\n",
    "    - ncols (int): Number of columns for the subplot grid. Default is 1.\n",
    "    - **kwargs: Additional common keyword arguments passed to the plotting function.\n",
    "\n",
    "    Returns:\n",
    "    - fig (matplotlib.figure.Figure): The created figure object.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    width = 18 if ncols == 1 else ncols * 5.43\n",
    "    height = 6 if nrows == 1 else nrows * 4\n",
    "    \n",
    "    plt.figure(figsize=(width, height))\n",
    "    \n",
    "    # print(axes)\n",
    "    \n",
    "    # Loop through chart_objs to plot the respective data\n",
    "    for i, chart in enumerate(chart_objs, 1):\n",
    "\n",
    "        plot_function = chart['plot_function']\n",
    "        title = chart['title']\n",
    "        xlabel = chart.get('xlabel', None)\n",
    "        ylabel = chart.get('ylabel', None)\n",
    "        x = chart.get('x', None)\n",
    "        y = chart.get('y', None)\n",
    "        data = chart.get('data', None)\n",
    "        chart_kwargs = chart.get('kwargs', {})\n",
    "\n",
    "        if x is None and data is None:\n",
    "            return \"Please provide either value of data or x\"\n",
    "        \n",
    "        plt.subplot(nrows, ncols, i)\n",
    "        \n",
    "        # Construct the plotting function arguments\n",
    "        plot_args = {\n",
    "            'data': data,\n",
    "            'x': x,\n",
    "        }\n",
    "       \n",
    "        if y is not None:\n",
    "            plot_args['y'] = y\n",
    "        \n",
    "        # Add any additional keyword arguments for this specific chart\n",
    "        plot_args.update(chart_kwargs)\n",
    "        \n",
    "        # Call the plot function\n",
    "        ax = plot_function(**plot_args)\n",
    "\n",
    "        plt.title(title, fontsize=16, pad=20)\n",
    "\n",
    "        if xlabel is not None:\n",
    "            plt.xlabel(xlabel)\n",
    "\n",
    "        if ylabel is not None:\n",
    "            plt.ylabel(ylabel)\n",
    "\n",
    "\n",
    "        if plot_function in [plot_functions['bar'], plot_functions['count']]:\n",
    "            for i in ax.containers:\n",
    "                ax.bar_label(i, fmt='%.2f')\n",
    "\n",
    "    # Adjust layout for better readability\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig = plt.gcf()\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f30dfb-2e27-460a-b5ba-0e9273fd3b4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### DataFrame - Utitlity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b27b48-f8ff-4f1e-ac33-5f717b3ecf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_extension_from_path(input_path):\n",
    "    # Use Path on system path (e.g., relative or absolute)\n",
    "    file_extension = Path(input_path).suffix.lstrip('.')\n",
    "    return file_extension.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "873f4e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataframe(data_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads raw data from a URL or file path and creates a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data_url (str): URL or file path of the raw data (e.g., GitHub raw file).\n",
    "        data_type (str): The type of the data ('csv', 'xlsx', 'json', 'parquet').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the data.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Map data types to pandas functions\n",
    "    read_functions = {\n",
    "        \"csv\": pd.read_csv,\n",
    "        \"xlsx\": pd.read_excel,\n",
    "        \"json\": pd.read_json,\n",
    "        \"parquet\": pd.read_parquet,\n",
    "    }\n",
    "\n",
    "    data_type = get_file_extension_from_path(data_url)\n",
    "    \n",
    "    if data_type not in read_functions:\n",
    "        raise ValueError(f\"Unsupported data type: {data_type}\")\n",
    "\n",
    "    try:\n",
    "        # Use the appropriate pandas function to read the data\n",
    "        df = read_functions[data_type](data_url)\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to create DataFrame: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d619ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(df, exclude_columns=None):\n",
    "    \"\"\"\n",
    "    Performs analysis on the dataset and stores the results.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to analyze.\n",
    "        exclude_columns (list): List of columns to exclude from summary statistics (optional).\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with the results of the analysis.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # General dataset info\n",
    "    results['rows'], results['columns'] = df.shape\n",
    "\n",
    "    # Missing values\n",
    "    missing_count = df.isnull().sum()\n",
    "    missing_details = missing_count[missing_count > 0].to_dict()\n",
    "    total_missing = missing_count.sum()\n",
    "    results['missing_values'] = {\n",
    "        'total': total_missing,\n",
    "        'percentage': (total_missing / (results['rows'] * results['columns'])) * 100,\n",
    "        'details': missing_details\n",
    "    }\n",
    "\n",
    "    # Duplicate rows\n",
    "    results['duplicate_rows'] = df.duplicated().sum()\n",
    "\n",
    "    # Data types\n",
    "    results['data_types'] = df.dtypes.reset_index()\n",
    "    results['data_types'].columns = ['Column', 'DataType']\n",
    "\n",
    "    # Exclude columns from analysis (if provided)\n",
    "    numeric_df = df.select_dtypes(include=['number'])\n",
    "    if exclude_columns:\n",
    "        numeric_df = numeric_df.drop(columns=exclude_columns, errors='ignore')\n",
    "\n",
    "    # Summary statistics\n",
    "    results['statistics'] = numeric_df.describe().T\n",
    "    results['statistics'] = results['statistics'].round(2)  # Format statistics to 2 decimal points\n",
    "\n",
    "    # Additional observations for each column\n",
    "    observations = {}\n",
    "    for col in results['statistics'].index:\n",
    "        stats = results['statistics'].loc[col]\n",
    "        mean = stats['mean']\n",
    "        median = stats['50%']\n",
    "        if mean < median:\n",
    "            observations[col] = \"Data is left-skewed (mean < median).\"\n",
    "        elif mean > median:\n",
    "            observations[col] = \"Data is right-skewed (mean > median).\"\n",
    "        else:\n",
    "            observations[col] = \"Data is symmetric (mean â‰ˆ median).\"\n",
    "\n",
    "    results['observations'] = observations\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a602e0a-d7ec-45ac-9dbd-cf5e63e4874e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Correlation Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a9d26b3-6b3b-4730-a73e-68672ad9cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_correlation_matrix(df, columns=None, correlation_matrix=None):\n",
    "    \"\"\"\n",
    "    Visualize the correlation matrix of numeric columns using a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to analyze and visualize.\n",
    "        columns (list, optional): List of columns to calculate correlation matrix. \n",
    "                                  If None, the entire DataFrame will be used.\n",
    "        correlation_matrix (pd.DataFrame, optional): Precomputed correlation matrix to visualize. \n",
    "                                                     If None, the correlation matrix will be computed from 'columns'.\n",
    "    \n",
    "    Returns:\n",
    "        fig: The generated figure object.\n",
    "    \"\"\"\n",
    "    # Compute the correlation matrix based on the inputs (columns or precomputed correlation_matrix)\n",
    "    if correlation_matrix is None:\n",
    "        # If correlation_matrix is not provided, compute it from the given columns or all numeric columns\n",
    "        numeric_columns = columns if columns else df.select_dtypes(include=['number']).columns\n",
    "        correlation_matrix = df[numeric_columns].corr(numeric_only=True)\n",
    "    \n",
    "    # Create chart_objs with the heatmap visualization for the correlation matrix\n",
    "    chart_objs = [{\n",
    "        'plot_function': plot_functions['heatmap'],\n",
    "        'title': 'Relationship between Variables: Correlation Matrix',\n",
    "        'xlabel': 'Features',\n",
    "        'ylabel': 'Features',\n",
    "        'x': correlation_matrix,  # Using the correlation matrix for visualization\n",
    "        'kwargs': {\n",
    "            'annot': True,  # Show correlation coefficients in the heatmap\n",
    "            'cmap': 'viridis',  # Color map for the heatmap\n",
    "            'fmt': '.2f',  # Format for the correlation values\n",
    "            'linewidths': 0.7,  # Line thickness between cells\n",
    "            'vmin': -1,\n",
    "            'vmax': 1\n",
    "            # 'cbar_kws': {'shrink': 0.75}  # Color bar size adjustment\n",
    "        }\n",
    "    }]\n",
    "    \n",
    "    # Visualize the correlation matrix using the visualize_chart function\n",
    "    return visualize_chart(chart_objs, nrows=1, ncols=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d751680-c090-4e9f-a8dd-899283a7c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correlation_matrix(df, columns=None, threshold=None):\n",
    "    \"\"\"\n",
    "    Analyze the correlation matrix of a DataFrame and optionally filter correlations by a threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to analyze.\n",
    "        columns (list, optional): List of columns to include in the correlation analysis. If None, all numeric columns are used.\n",
    "        threshold (float, optional): Correlation threshold to filter significant correlations. If None, no filtering is applied.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the full correlation matrix and the filtered correlation matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use only the specified columns or default to all numeric columns\n",
    "    if columns is not None:\n",
    "        # Check if the provided columns are numeric\n",
    "        non_numeric_columns = [col for col in columns if df[col].dtype not in ['int64', 'float64']]\n",
    "        \n",
    "        if non_numeric_columns:\n",
    "            print(f\"Warning: The following non-numeric columns were excluded from skewness analysis: {', '.join(non_numeric_columns)}\")\n",
    "            # Remove non-numeric columns from the columns list\n",
    "            columns = [col for col in columns if col not in non_numeric_columns]\n",
    "        \n",
    "    else:\n",
    "        columns = df.select_dtypes(include='number').columns.tolist()\n",
    "    \n",
    "    if not columns:\n",
    "        raise ValueError(\"No numeric columns available for correlation analysis.\")\n",
    "\n",
    "    # Compute the correlation matrix for the selected columns\n",
    "    filtered_df = df[columns]\n",
    "    correlation_matrix = filtered_df.corr()\n",
    "\n",
    "    # Filter the correlation matrix based on the threshold\n",
    "    if threshold is not None:\n",
    "        filtered_matrix = correlation_matrix[abs(correlation_matrix) >= threshold].fillna(0)\n",
    "    else:\n",
    "        filtered_matrix = correlation_matrix  # No filtering applied if threshold is None\n",
    "\n",
    "\n",
    "    visualize_correlation_matrix(df, correlation_matrix=correlation_matrix)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "    # Return both the full and filtered correlation matrices\n",
    "    return {\n",
    "        \"correlation_matrix\": correlation_matrix,\n",
    "        \"filtered_correlation_matrix\": filtered_matrix\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce759a-c892-45f6-8111-fe847889a147",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Skewness Utitlity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce2f2798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_skewness(df, columns=None):\n",
    "    \"\"\"\n",
    "    Calculate skewness for specific columns or all numeric columns in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to analyze.\n",
    "        columns (list): A list of column names for which skewness needs to be calculated. \n",
    "                          If None, skewness will be calculated for all numeric columns.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with column names as keys and skewness values as values.\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        # If no column names are provided, calculate skewness for all numeric columns\n",
    "        numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    else:\n",
    "        # Check if the provided columns are numeric\n",
    "        non_numeric_columns = [col for col in columns if df[col].dtype not in ['int64', 'float64']]\n",
    "        \n",
    "        if non_numeric_columns:\n",
    "            print(f\"Warning: The following non-numeric columns were excluded from skewness analysis: {', '.join(non_numeric_columns)}\")\n",
    "            # Remove non-numeric columns from the columns list\n",
    "            columns = [col for col in columns if col not in non_numeric_columns]\n",
    "\n",
    "    # Calculate skewness for the selected columns\n",
    "    skewness_dict = {col: df[col].skew() for col in numeric_columns}\n",
    "    return skewness_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b04e6f7-84b1-4e37-9468-a8a4ebf4aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_skewness_with_chart(df, numeric_columns):\n",
    "    \"\"\"\n",
    "    Visualize skewness of all numeric columns using histograms with KDE.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to analyze and visualize.\n",
    "    \n",
    "    Returns:\n",
    "        fig: The generated figure object.\n",
    "    \"\"\"\n",
    "    # Identify numeric columns\n",
    "    # numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    # Calculate skewness for each numeric column\n",
    "    skewness = df[numeric_columns].skew().round(2)\n",
    "\n",
    "    # print(skewness)\n",
    "    \n",
    "    # Create chart_objs with customized titles and other options for each column\n",
    "    chart_objs = []\n",
    "    for col in numeric_columns:\n",
    "        # print(col, skewness[col])\n",
    "        chart_objs.append({\n",
    "            'plot_function':plot_functions['hist'] ,\n",
    "            'title': f'Skewness of {col}: {skewness[col]}',  # Title for each individual column\n",
    "            'xlabel': col,\n",
    "            'ylabel': 'Frequency',\n",
    "            'x': df[col],\n",
    "            'kwargs': {'kde': True, 'color': 'purple', 'element': 'poly'}\n",
    "        })\n",
    "\n",
    "\n",
    "    # print(chart_objs)\n",
    "    \n",
    "    # Visualize skewness using the visualize_chart function\n",
    "    return visualize_chart(chart_objs, nrows=(len(numeric_columns) // 3 + 1), ncols=3)\n",
    "\n",
    "# # Example: Visualize the skewness of all numeric columns in the DataFrame\n",
    "# fig = visualize_skewness_with_chart(df)\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44213569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_skewness(df, columns=None):\n",
    "    \"\"\"\n",
    "    Analyze the skewness of numeric columns in the dataset and generate a summary of skewness categories.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to analyze.\n",
    "        columns (list, optional): List of columns to include in the skewness analysis. \n",
    "                                   If None, all numeric columns will be considered.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the skewness analysis result categorized by 'high', 'moderate', and 'low'.\n",
    "        plt.Figure: A Matplotlib figure object displaying histograms of the skewness.\n",
    "    \"\"\"\n",
    "    # Determine the columns to analyze (use all numeric columns if no specific columns are provided)\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "    \n",
    "    # Check if the provided columns are numeric\n",
    "    non_numeric_columns = [col for col in columns if df[col].dtype not in ['int64', 'float64']]\n",
    "    \n",
    "    if non_numeric_columns:\n",
    "        print(f\"Warning: The following non-numeric columns were excluded from skewness analysis: {', '.join(non_numeric_columns)}\")\n",
    "        # Remove non-numeric columns from the columns list\n",
    "        columns = [col for col in columns if col not in non_numeric_columns]\n",
    "\n",
    "    # If there are no numeric columns to analyze, raise an error\n",
    "    if not columns:\n",
    "        raise ValueError(\"No numeric columns available in the dataset for skewness analysis.\")\n",
    "    \n",
    "    # Calculate skewness for each numeric column\n",
    "    skewness_values = df[columns].apply(lambda x: x.skew()).to_dict()\n",
    "\n",
    "    # Categorize columns based on skewness values\n",
    "    high_skew = [col for col, skew in skewness_values.items() if abs(skew) > 1]\n",
    "    moderate_skew = [col for col, skew in skewness_values.items() if 0.5 < abs(skew) <= 1]\n",
    "    low_skew = [col for col, skew in skewness_values.items() if abs(skew) <= 0.5]\n",
    "\n",
    "    # Prepare the result object\n",
    "    skewness_result = {\n",
    "        \"high_skew\": high_skew,\n",
    "        \"moderate_skew\": moderate_skew,\n",
    "        \"low_skew\": low_skew,\n",
    "        \"skewness_values\": skewness_values  # For detailed skewness values of each column\n",
    "    }\n",
    "\n",
    "    # print(skewness_result)\n",
    "    # Generate and visualize skewness distribution for the specified columns\n",
    "    visualize_skewness_with_chart(df, columns)\n",
    "\n",
    "    return skewness_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06545658-10d4-46b9-8345-3108501840a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### DataFrame - Manipulation - Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51c4f08-e300-42d7-94ba-ba5325883113",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Dtype - conversion _(changing column dtype)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d32f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_columns(df, columns, new_type):\n",
    "    \"\"\"\n",
    "    Converts specified columns to a new data type.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to transform.\n",
    "        columns (list): List of column names to convert.\n",
    "        new_type (type): The new data type (e.g., int, float, str).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with updated column types.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        try:\n",
    "            df[column] = df[column].astype(new_type)\n",
    "            print(f\"Column '{column}' successfully converted to {new_type}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting column '{column}' to {new_type}: {e}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a0a8da-e121-4dec-8a27-e3c1c42b07be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Column Imputation -- _(Handling missing values)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "492cb1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_column(col, method):\n",
    "    \"\"\"\n",
    "    Impute missing values in a column based on the specified method.\n",
    "    \n",
    "    Args:\n",
    "        col (pd.Series): The column to impute.\n",
    "        method (str): The imputation method ('median', 'mean', or 'mode').\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: The column with imputed values.\n",
    "    \"\"\"\n",
    "    if method == 'median':\n",
    "        return col.fillna(col.median())\n",
    "    elif method == 'mean':\n",
    "        return col.fillna(col.mean())\n",
    "    elif method == 'mode':\n",
    "        return col.fillna(col.mode()[0])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported imputation method: {method}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50e0a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing(df, exclude_columns=None):\n",
    "    \"\"\"\n",
    "    Impute missing values for numeric, categorical, datetime, and boolean columns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset to process.\n",
    "        exclude_columns (list): Columns to exclude from the imputation process.\n",
    "        \n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with imputed missing values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If no exclude_columns are provided, initialize as an empty list\n",
    "    exclude_columns = exclude_columns or []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col not in exclude_columns and df[col].isnull().any():  # Check if the column has missing values\n",
    "            dtype = df[col].dtype\n",
    "\n",
    "            # Handle numeric columns\n",
    "            if dtype in ['int64', 'float64']:\n",
    "                skew = df[col].skew()  # Calculate skewness for numeric columns\n",
    "                if abs(skew) > 0.5:\n",
    "                    df[col] = impute_column(df[col], 'median')\n",
    "                else:\n",
    "                    df[col] = impute_column(df[col], 'mean')\n",
    "            # Handle categorical, boolean, and datetime columns\n",
    "            elif dtype in ['object', 'category', 'bool', 'datetime64[ns]']:\n",
    "                df[col] = impute_column(df[col], 'mode')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aefe767-60b7-42cd-9389-2494f810d92b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e2662e5-7d92-4311-bdc3-6a667ee91719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, col, lower, upper):\n",
    "    \"\"\"Remove rows with outliers for the given column.\"\"\"\n",
    "    return df[(df[col] >= lower) & (df[col] <= upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b2cc969-df33-4be8-a5e0-1a39ad0e8f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_outliers(df, col, lower, upper):\n",
    "    \"\"\"Flag outliers for the given column by adding a new column.\"\"\"\n",
    "    df[f\"{col}_outlier\"] = (df[col] < lower) | (df[col] > upper)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f78fa510-2acc-4d80-91a5-68355a31cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_outliers(df, col, lower, upper):\n",
    "    \"\"\"Clip outliers for the given column to the bounds.\"\"\"\n",
    "    df[col] = df[col].clip(lower=lower, upper=upper)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2970aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers_in_data(df, columns=None, method='remove', threshold=1.5):\n",
    "    \"\"\"\n",
    "    Detect and handle outliers in the DataFrame using the IQR method without using if-else.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to check for outliers.\n",
    "        columns (list, optional): List of columns to check for outliers. If None, all numeric columns are used.\n",
    "        method (str): Method to handle outliers - 'remove', 'flag', or 'transform'.\n",
    "        threshold (float): The IQR multiplier for detecting outliers.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with outliers handled (removed, flagged, or transformed).\n",
    "        pd.DataFrame: DataFrame containing only the outliers.\n",
    "    \"\"\"\n",
    "    # Select numeric columns if no specific columns are provided\n",
    "    columns = columns or df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "    # DataFrames for results\n",
    "    df_result = df.copy()\n",
    "    outliers_list = []\n",
    "\n",
    "    # Map methods to functions\n",
    "    method_actions = {\n",
    "        'remove': remove_outliers,\n",
    "        'flag': flag_outliers,\n",
    "        'transform': transform_outliers,\n",
    "    }\n",
    "\n",
    "    # Raise error for invalid method\n",
    "    if method not in method_actions:\n",
    "        raise ValueError(f\"Invalid method '{method}'. Choose from 'remove', 'flag', or 'transform'.\")\n",
    "\n",
    "    # Loop through each column and handle outliers\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in the DataFrame. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Calculate IQR bounds\n",
    "        Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - threshold * IQR\n",
    "        upper_bound = Q3 + threshold * IQR\n",
    "\n",
    "        # Identify outliers\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outliers_list.append(outliers)\n",
    "\n",
    "        print(f\"Outliers detected for '{col}': {len(outliers)}\")\n",
    "\n",
    "        # Call the appropriate method\n",
    "        df_result = method_actions[method](df_result, col, lower_bound, upper_bound)\n",
    "\n",
    "    # Concatenate all outliers into a single DataFrame\n",
    "    df_outliers = pd.concat(outliers_list).drop_duplicates()\n",
    "\n",
    "    return df_result, df_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c4928e-824b-4c8d-a271-83b7ac1ceac0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Data Transformation - _(log, sqrt)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3848cc5a-ab68-447a-b1a1-41cf1f4b7056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformation(df, column, transformation_type):\n",
    "    \"\"\"\n",
    "    Apply the specified transformation to a column in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the column to transform.\n",
    "        column (str): The name of the column to transform.\n",
    "        transformation_type (str): The type of transformation to apply. \n",
    "                                   Supported: 'log', 'sqrt', 'square', 'reciprocal'.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Transformed column as a pandas Series.\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n",
    "\n",
    "    # Transformation mapping\n",
    "    transformations = {\n",
    "        'log': lambda x: np.log1p(x),\n",
    "        'sqrt': lambda x: np.sqrt(x.clip(lower=0)),\n",
    "        'square': lambda x: np.square(x),\n",
    "        'reciprocal': lambda x: 1 / x.replace(0, np.nan)\n",
    "    }\n",
    "\n",
    "    # Get the transformation function\n",
    "    transform_func = transformations.get(transformation_type)\n",
    "    if not transform_func:\n",
    "        raise ValueError(f\"Unsupported transformation type: '{transformation_type}'.\")\n",
    "\n",
    "    # Apply the transformation\n",
    "    return transform_func(df[column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b546b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_transformation_with_outliers(df, skew_categories, handle_outliers=False):\n",
    "    \"\"\"\n",
    "    Apply the best transformation based on skewness for each column already categorized into high, moderate, or low skew.\n",
    "    Optionally, handle outliers by removing or transforming them.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing numeric columns.\n",
    "        skew_categories (dict): Dictionary with keys 'high', 'moderate', 'low' mapping to lists of column names.\n",
    "        handle_outliers (bool): Whether to handle outliers by removal after transformation.\n",
    "        skew_threshold (float): The threshold for determining the degree of skewness for transformation.\n",
    "        outlier_threshold (float): The threshold for detecting outliers based on IQR method.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with transformations applied to columns and outliers handled (if applicable).\n",
    "    \"\"\"\n",
    "    transformed_df = df.copy()  # Create a copy to apply transformations\n",
    "\n",
    "    # Dictionary of transformations for each skew category\n",
    "    transformations = {\n",
    "        'high_skew': 'log',\n",
    "        'moderate_skew': 'sqrt'\n",
    "    }\n",
    "    \n",
    "    # 'low': 'boxcox'  # Optional, or we can skip transformation for low skew\n",
    "\n",
    "    transformed_col = []\n",
    "    # Apply transformations based on skew category\n",
    "    for skew_category, transformation_type in transformations.items():\n",
    "        columns = skew_categories.get(skew_category, [])\n",
    "        for col in columns:\n",
    "            print(f\"Applying {transformation_type} transformation to {col} due to {skew_category} skewness.\")\n",
    "            transformed_df[col] = apply_transformation(transformed_df, col, transformation_type)\n",
    "            transformed_col.append(col)\n",
    "\n",
    "    \n",
    "    # Handle outliers if required\n",
    "    if handle_outliers:\n",
    "        transformed_df = handle_outliers_in_data(transformed_df, transformed_col)\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2687a3a-00d7-47c7-949f-85ab48c81604",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Summary Generation -- _(Factory class to register and use summary related functions)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54bc490e-b02f-40b5-94e1-50ffa6776389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Factory Class for Summaries\n",
    "class SummaryFactory:\n",
    "    def __init__(self):\n",
    "        self.steps = {}\n",
    "\n",
    "    def register_step(self, step_name, step_function):\n",
    "        \"\"\"\n",
    "        Registers a summary step to the factory.\n",
    "        \n",
    "        Args:\n",
    "            step_name (str): The name of the step.\n",
    "            step_function (function): The function to generate the summary for this step.\n",
    "        \"\"\"\n",
    "        self.steps[step_name] = step_function\n",
    "\n",
    "    def generate_summary(self, step_name, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Generates the summary for the specified step.\n",
    "        \n",
    "        Args:\n",
    "            step_name (str): The name of the step to execute.\n",
    "            *args: Positional arguments for the step function.\n",
    "            **kwargs: Keyword arguments for the step function.\n",
    "        \n",
    "        Returns:\n",
    "            str: The generated summary.\n",
    "        \"\"\"\n",
    "        if step_name not in self.steps:\n",
    "            raise ValueError(f\"Step '{step_name}' is not registered in the factory.\")\n",
    "        return self.steps[step_name](*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a65201b-0ef1-4321-b467-e9845e8f562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Overview Function\n",
    "def overview_step(result):\n",
    "    \"\"\"\n",
    "    Generates an overview summary from the dataset analysis results.\n",
    "    \n",
    "    Args:\n",
    "        result (dict): The analysis results dictionary.\n",
    "    \n",
    "    Returns:\n",
    "        str: Overview summary.\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "\n",
    "    # General dataset information\n",
    "    summary.append(f\"The dataset contains {result['rows']} rows and {result['columns']} columns.\\n\")\n",
    "\n",
    "    # Missing values\n",
    "    missing_values = result['missing_values']\n",
    "    summary.append(f\"There are {missing_values['total']} missing values across {len(missing_values['details'])} columns.\")\n",
    "    summary.append(f\"Missing values account for {missing_values['percentage']:.2f}% of the dataset.\")\n",
    "    if missing_values['details']:\n",
    "        summary.append(\"Columns with missing values and their counts:\")\n",
    "        for col, count in missing_values['details'].items():\n",
    "            summary.append(f\"  - {col}: {count} missing values\")\n",
    "    summary.append(\"\")  # Add a blank line for spacing\n",
    "\n",
    "    # Duplicate rows\n",
    "    duplicate_rows = result['duplicate_rows']\n",
    "    if duplicate_rows > 0:\n",
    "        summary.append(f\"There are {duplicate_rows} duplicate rows in the dataset.\")\n",
    "    else:\n",
    "        summary.append(\"There are no duplicate rows in the dataset.\")\n",
    "    summary.append(\"\")  # Add a blank line for spacing\n",
    "\n",
    "    # Data types\n",
    "    summary.append(\"Data Types:\\n\")\n",
    "    data_types = result['data_types']\n",
    "    \n",
    "    summary.append(data_types.to_string(index=False))\n",
    "\n",
    "    # for _, row in data_types.iterrows():\n",
    "    #     summary.append(f\"  - {row['Column']}: {row['DataType']}\")\n",
    "    \n",
    "    summary.append(\"\")  # Add a blank line for spacing\n",
    "\n",
    "    # Summary Statistics\n",
    "    summary.append(\"Summary Statistics:\\n\")\n",
    "    statistics = result['statistics']\n",
    "    summary.append(statistics.to_string())  # Use pandas' `to_string` for a clean table-like output\n",
    "\n",
    "    return \"\\n\".join(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efebe2f0-201e-4843-9cdb-e6914734255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Observations Function\n",
    "def observations_step(result):\n",
    "    \"\"\"\n",
    "    Generates observations based on numerical analysis.\n",
    "    \n",
    "    Args:\n",
    "        result (dict): The analysis results dictionary.\n",
    "    \n",
    "    Returns:\n",
    "        str: Observations summary.\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "\n",
    "    # Observations from numerical columns\n",
    "    summary.append(\"Observations based on the dataset:\\n\")\n",
    "    observations = result.get('observations', {})\n",
    "    \n",
    "    for col, observation in observations.items():\n",
    "        summary.append(f\"  - {col}: {observation}\")\n",
    "\n",
    "    return \"\\n\".join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78673406-4373-4a1a-86ee-347350cc0e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewness_summary(skewness_object):\n",
    "    \"\"\"\n",
    "    Generate a textual and tabular summary of skewness using the skewness object.\n",
    "\n",
    "    Args:\n",
    "        skewness_object (dict): A dictionary containing skewness analysis results.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted textual and tabular summary of the skewness analysis.\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "\n",
    "    # Categorize and format the summary based on the skewness object\n",
    "    if skewness_object.get(\"high_skew\"):\n",
    "        summary.append(f\"- Highly skewed columns (suggesting log transformation): {', '.join(skewness_object['high_skew'])}\")\n",
    "    if skewness_object.get(\"moderate_skew\"):\n",
    "        summary.append(f\"- Moderately skewed columns (suggesting square root transformation): {', '.join(skewness_object['moderate_skew'])}\")\n",
    "    if skewness_object.get(\"low_skew\"):\n",
    "        summary.append(f\"- Columns with low skewness (no transformation needed): {', '.join(skewness_object['low_skew'])}\")\n",
    "    \n",
    "    # Convert detailed skewness values into a table\n",
    "    if skewness_object.get(\"skewness_values\"):\n",
    "        skewness_df = pd.DataFrame.from_dict(skewness_object[\"skewness_values\"], orient=\"index\", columns=[\"Skewness\"])\n",
    "        skewness_df.index.name = \"Column\"\n",
    "        skewness_df.reset_index(inplace=True)\n",
    "        summary.append(\"\\nDetailed Skewness Values:\\n\")\n",
    "        summary.append(skewness_df.to_string(index=False))\n",
    "\n",
    "    return \"\\n\".join(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48685681-ddca-4328-a0fe-ed850fa004e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_factory = SummaryFactory()\n",
    "\n",
    "# Register steps\n",
    "summary_factory.register_step(\"overview\", overview_step)\n",
    "summary_factory.register_step(\"observations\", observations_step)\n",
    "summary_factory.register_step(\"skewness\", skewness_summary)\n",
    "\n",
    "# print(summary_factory.generate_summary(\"overview\", result)) # to generate summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61628a90-8df7-4e39-bd59-d32c6e75c3f3",
   "metadata": {},
   "source": [
    "### Custom Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82951d5f-81ab-42c9-853f-f0715df43e28",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ebf545-0249-4711-9953-a123d41a3b5c",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e29df2-d5d7-47af-a28f-9d5dc89a9f0f",
   "metadata": {},
   "source": [
    "#### Load the Netflix dataset (CSV file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ce0e07d-5916-4d13-a603-370e32240068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe\n",
    "\n",
    "df = create_dataframe(data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d448f9a-744e-41db-b9de-8dd6fe1d9d91",
   "metadata": {},
   "source": [
    "#### First View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51ce56-b3f9-4c93-9ed5-9797db27e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277b795-03b5-4cd4-8ac0-ccadbe393eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0609d066-b50f-4daf-a228-a4dab0980230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding non-numeric and columns that are not needed in descriptive summary function like df.describe(), correlation and skewness etc\n",
    "\n",
    "exclude_columns = {'id'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8328f2-1c47-4959-b4e6-6e707a243177",
   "metadata": {},
   "source": [
    "#### Inspect the structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "014f6e39-01ab-4084-8745-b47b4bda6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = analyze_dataset(df, exclude_columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c5b6fe-6aba-4118-8f15-0145520c798d",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf28574-1246-4705-b721-acdddc571af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_factory.generate_summary(\"overview\", result))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(summary_factory.generate_summary(\"observations\", result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c91f1-b809-409f-ab7b-152cdc45e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe(include=['object']).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16183f7f-3e12-4101-97a4-9b2229b9f97d",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd49a05-c9ca-48c6-94cd-6cf73018def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21a033c-2be9-4fe8-9f2e-bf245818958f",
   "metadata": {},
   "source": [
    "**Handling Missig Values - Rating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c8784-84e4-4af5-8ae2-c2229a67202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19325f2c-e2d1-4ff3-8910-467805cd74f5",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b6dfa1-d86e-411a-895f-160c4c7f32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_column = list(set(df.columns) - exclude_columns)\n",
    "filter_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b58150-1bae-4e0f-b45f-4cac19134656",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = analyze_correlation_matrix(df, filter_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed184a0-f5f4-4764-a5e0-2076c7fecfa4",
   "metadata": {},
   "source": [
    "### Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c5a60-043a-47c1-8894-534399362a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = analyze_skewness(df, filter_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa75cc-dc73-4a1e-9414-1524a762437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_factory.generate_summary(\"skewness\", skewness))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c5dc5",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_objs = []\n",
    "for col in filter_column:\n",
    "    # print(col, skewness[col])\n",
    "    chart_objs.append({\n",
    "    'plot_function':plot_functions['box'] ,\n",
    "    'title': f'Boxplot of {col}',  # Title for each individual column\n",
    "    'xlabel': col,\n",
    "    'ylabel': None,\n",
    "    'x': df[col],\n",
    "    'kwargs': { 'color': 'purple'}\n",
    "    })\n",
    "    \n",
    "\n",
    "visualize_chart(chart_objs, nrows=(len(filter_column) // 3 + 1), ncols=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ff831",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1, outlier_df = best_transformation_with_outliers(df, skewness, handle_outliers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df.shape, df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de6f61-fd31-4acb-9062-e41149f9d01f",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356014a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d5bea5d",
   "metadata": {},
   "source": [
    "## Outliers - observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37c21b",
   "metadata": {},
   "source": [
    "#### Inspect the structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25124baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = analyze_dataset(outlier_df, exclude_columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a5b04",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a2018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_factory.generate_summary(\"overview\", result))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(summary_factory.generate_summary(\"observations\", result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd06b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(outlier_df.describe(include=['object']).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ccc45a",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7383b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13adf7",
   "metadata": {},
   "source": [
    "**Handling Missig Values - Rating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30c836",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21067bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_column = list(set(outlier_df.columns) - exclude_columns)\n",
    "filter_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = analyze_correlation_matrix(outlier_df, filter_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac109e25",
   "metadata": {},
   "source": [
    "### Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab437a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = analyze_skewness(outlier_df, filter_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e088cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_factory.generate_summary(\"skewness\", skewness))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(rex)",
   "language": "python",
   "name": "rex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
